# Emotion Recognition on Wav2Vec2
Классификация эмоций на аудио, используя эмбеддинги предобученной модели
<br>

##### Навигация:
- Отчёт с кодом - EmotionClassification.ipynb
- Инференс модели и подсчёт значений на тесте в файле inference.ipynb
- Сравнение моделей - comparing_models.ipynb
- Ответы на тесте - answers.txt
- В data хранятся веса обученной модели, извлечённые фичи на трейне, а также тестовый и обучающий датасеты

### Wav2vec

Wav2Vec2 - модель для распознования речи на аудио. Мы решаем немного другую задачу, но предобученная модель всё равно может нам в этом помочь.

Так как Wav2Vec2 обучилась на большом количестве аудио, в её скрытых слоях накопилась важная обощающая информация, которую можно использоваь как фичи для решения соверешенно других задач. То есть мы можем взять новые аудио, получить их эмбеддинги и на них решать задачу классфикации. Перед этим нужно только поменять частоту дискретизации на 16k и отнормализовать данные, т.к. в документации напрямую сказано, что часто это существенно улучшает качество моделей

### Датасет
Датасет RAVDESS состоит из аудио, озвученных 24 разными актёрами, в равной пропорции женщинами и мужчинами. Каждый актёр озвучил каждую фразу каждой эмоцией по два раза, правда разных фраз всего было две - "Kids are talking by the door" и "Dogs are sitting by the door". Помимо прочего, в датасете есть сила эмоции normal и strong, но для neutral эмоции strong нет. Из-за этого класс neutral несбалансирован, так что при разделениях датасета использовалась стратификация - чтобы в валидационную и тренировочную части попадали объекты равномерно из каждого класса.

### Классификация
Для решения задачи я сравнил несколько классификаторов, реализованных в sklearn. Градиентный бустинг сравнивался со случайным лесом и многослойной сетью. По всем им был приблизительный поиск по сетке гиперпараметров (сравнение проведено в файле comparing_models.ipynb). Наиболее эффективным оказался градиентный бустинг с accuracy 48%, что не является плохим результатом, т.к. меток классов 8, то есть "подбрасывание монетки" дало бы нам только 12.5%. В других работах на аналогичных данных достигается accuracy в 60% https://www.kaggle.com/code/shivamburnwal/speech-emotion-recognition

Графики accuracy градиентного бустинга по гиперпараметрам:
![img.png](imgs/img.png)

![img_1.png](imgs/img_1.png)

Итоговая модель справлялась с эмоциями злости, страха, удивления лучше всего. И хуже всего с грустью



### Что можно сделать ещё
Дополнительно увеличить наш датасет путём аугментаций. Дело в том, что 1.5к объектов - это не так и много, когда дело доходит до обучения сложных моделей. При этом метка класса не должна меняться от добавления шумов, небольшого изменения громкости или высоты звука, в то время, когда это существенно изменит эмбеддинг в Wav2Vec. Таким образом, мы получим новые данные.

Помимо этого интересно конвертировать аудио в спектрограммы и поприменять предобученные на картинках сети к ним. (Спектрограммы очень просто получаются библиотекой librose)
